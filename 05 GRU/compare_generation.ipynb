{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Сравнение RNN, LSTM и GRU: генерация\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "root = Path('.').resolve()\n",
        "sys.path.append(str(root / '03 RNN'))\n",
        "sys.path.append(str(root / '04 LSTM'))\n",
        "sys.path.append(str(root / '05 GRU'))\n",
        "\n",
        "from rnn import ElmanRNN\n",
        "from lstm import LSTMNextTokenGenerator\n",
        "from gru import GRUNextTokenGenerator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_vocab(text: str):\n",
        "    chars = sorted(set(text))\n",
        "    char2idx = {ch: i for i, ch in enumerate(chars)}\n",
        "    idx2char = {i: ch for ch, i in char2idx.items()}\n",
        "    return char2idx, idx2char\n",
        "\n",
        "def encode_text(text: str, char2idx: dict[str, int]) -> np.ndarray:\n",
        "    return np.array([char2idx[ch] for ch in text], dtype=int)\n",
        "\n",
        "def to_one_hot(indices: np.ndarray, vocab_size: int) -> np.ndarray:\n",
        "    T = len(indices)\n",
        "    one_hot = np.zeros((T, vocab_size), dtype=float)\n",
        "    one_hot[np.arange(T), indices] = 1.0\n",
        "    return one_hot\n",
        "\n",
        "def build_sequences_indices(indices: np.ndarray, seq_len: int, vocab_size: int):\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "    for start in range(len(indices) - seq_len):\n",
        "        x_idx = indices[start:start + seq_len]\n",
        "        y_idx = indices[start + 1:start + seq_len + 1]\n",
        "        X_list.append(to_one_hot(x_idx, vocab_size))\n",
        "        y_list.append(y_idx)\n",
        "    return np.stack(X_list), np.stack(y_list)\n",
        "\n",
        "def build_sequences_onehot(indices: np.ndarray, seq_len: int, vocab_size: int):\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "    for start in range(len(indices) - seq_len):\n",
        "        x_idx = indices[start:start + seq_len]\n",
        "        y_idx = indices[start + 1:start + seq_len + 1]\n",
        "        X_list.append(to_one_hot(x_idx, vocab_size))\n",
        "        y_list.append(to_one_hot(y_idx, vocab_size))\n",
        "    return X_list, y_list\n",
        "\n",
        "def decode_indices(indices: np.ndarray, idx2char: dict[int, str]) -> str:\n",
        "    return ''.join(idx2char[i] for i in indices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_text = (\n",
        "    'to be or not to be that is the question whether tis nobler in the mind '\n",
        "    'to suffer the slings and arrows of outrageous fortune '\n",
        "    'or to take arms against a sea of troubles and by opposing end them '\n",
        "    'to die to sleep no more and by a sleep to say we end the heartache '\n",
        "    'and the thousand natural shocks that flesh is heir to '\n",
        ")\n",
        "\n",
        "repeats = 15\n",
        "text = base_text * repeats\n",
        "\n",
        "seq_len = 20\n",
        "\n",
        "char2idx, idx2char = build_vocab(text)\n",
        "vocab_size = len(char2idx)\n",
        "indices = encode_text(text, char2idx)\n",
        "\n",
        "split = int(len(indices) * 0.8)\n",
        "train_idx = indices[:split]\n",
        "test_idx = indices[split:]\n",
        "\n",
        "X_train, y_train = build_sequences_indices(train_idx, seq_len, vocab_size)\n",
        "X_test, y_test = build_sequences_indices(test_idx, seq_len, vocab_size)\n",
        "\n",
        "X_train_rnn, y_train_rnn = build_sequences_onehot(train_idx, seq_len, vocab_size)\n",
        "X_test_rnn, y_test_rnn = build_sequences_onehot(test_idx, seq_len, vocab_size)\n",
        "y_test_rnn_idx = [y.argmax(axis=1) for y in y_test_rnn]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_next_char_accuracy_lm(model, X_eval, y_eval_idx):\n",
        "    probs = model.predict_proba(X_eval)\n",
        "    preds = np.argmax(probs, axis=2)\n",
        "    correct = (preds == y_eval_idx).sum()\n",
        "    total = y_eval_idx.size\n",
        "    return correct / total\n",
        "\n",
        "def eval_next_char_accuracy_rnn(model, X_eval_list, y_eval_idx_list):\n",
        "    preds_list = model.predict(X_eval_list)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for y_idx, preds in zip(y_eval_idx_list, preds_list):\n",
        "        correct += np.sum(preds == y_idx)\n",
        "        total += len(y_idx)\n",
        "    return correct / total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RNN test accuracy: 0.8493\n"
          ]
        }
      ],
      "source": [
        "rnn = ElmanRNN(\n",
        "    input_size=vocab_size,\n",
        "    hidden_size=128,\n",
        "    output_size=vocab_size,\n",
        "    lr=1e-3,\n",
        "    epochs=20,\n",
        "    grad_clip=5.0,\n",
        "    random_state=42,\n",
        "    verbose=False,\n",
        ")\n",
        "rnn.fit(X_train_rnn, y_train_rnn)\n",
        "rnn_acc = eval_next_char_accuracy_rnn(rnn, X_test_rnn, y_test_rnn_idx)\n",
        "print(f'RNN test accuracy: {rnn_acc:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LSTM test accuracy: 0.8980\n"
          ]
        }
      ],
      "source": [
        "lstm = LSTMNextTokenGenerator(\n",
        "    input_dim=vocab_size,\n",
        "    hidden_dim=128,\n",
        "    vocab_size=vocab_size,\n",
        "    lr=1e-3,\n",
        "    max_epochs=20,\n",
        "    random_state=42,\n",
        "    verbose=False,\n",
        ")\n",
        "lstm.fit(X_train, y_train)\n",
        "lstm_acc = eval_next_char_accuracy_lm(lstm, X_test, y_test)\n",
        "print(f'LSTM test accuracy: {lstm_acc:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GRU test accuracy: 0.9347\n"
          ]
        }
      ],
      "source": [
        "gru = GRUNextTokenGenerator(\n",
        "    input_dim=vocab_size,\n",
        "    hidden_dim=128,\n",
        "    vocab_size=vocab_size,\n",
        "    lr=1e-3,\n",
        "    max_epochs=20,\n",
        "    random_state=42,\n",
        "    verbose=False,\n",
        ")\n",
        "gru.fit(X_train, y_train)\n",
        "gru_acc = eval_next_char_accuracy_lm(gru, X_test, y_test)\n",
        "print(f'GRU test accuracy: {gru_acc:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RNN sample:\n",
            "to be or not to be that ns sriis not wheep iiir to dor to sayawe and tha tiousand na more and by a aleip to eoke and bata aleshinp tofe and mhe heal ou os fs npblanrt ag ouerpinst ogeep to be mrar ouming ar wn the mfae \n",
            "LSTM sample:\n",
            "to be or not to be that is the question whe era oh the toro ne ta de that flesh is heir to to be or not to be that is the question whether to to be or not to be that is the question whe rling ondsh that flesh an whearmi\n",
            "GRU sample:\n",
            "to be or not to be that is the question whether tis nobler in the mind to suffer the slings and arrows of outrageous fortune or to take arms against a sea of troubles and by opposing end them to die to sleep no more and\n"
          ]
        }
      ],
      "source": [
        "def sample_lstm_like(model, seed_text: str, steps: int = 200):\n",
        "    prefix = seed_text\n",
        "    rng = np.random.default_rng(123)\n",
        "    for _ in range(steps):\n",
        "        window = prefix[-seq_len:]\n",
        "        x_idx = np.array([char2idx[ch] for ch in window])\n",
        "        x_vec = to_one_hot(x_idx, vocab_size)\n",
        "        probs = model.predict_next_proba(x_vec)\n",
        "        next_idx = rng.choice(vocab_size, p=probs)\n",
        "        prefix += idx2char[next_idx]\n",
        "    return prefix\n",
        "\n",
        "def sample_rnn(model, seed_text: str, steps: int = 200):\n",
        "    prefix = seed_text\n",
        "    rng = np.random.default_rng(123)\n",
        "    for _ in range(steps):\n",
        "        window = prefix[-seq_len:]\n",
        "        x_idx = np.array([char2idx[ch] for ch in window])\n",
        "        x_vec = to_one_hot(x_idx, vocab_size)\n",
        "        probs_seq = model.predict_proba([x_vec])[0]\n",
        "        probs = probs_seq[-1]\n",
        "        next_idx = rng.choice(vocab_size, p=probs)\n",
        "        prefix += idx2char[next_idx]\n",
        "    return prefix\n",
        "\n",
        "print('RNN sample:')\n",
        "print(sample_rnn(rnn, seed_text='to be or not to be '))\n",
        "\n",
        "print('LSTM sample:')\n",
        "print(sample_lstm_like(lstm, seed_text='to be or not to be '))\n",
        "\n",
        "print('GRU sample:')\n",
        "print(sample_lstm_like(gru, seed_text='to be or not to be '))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
